# üöÄ Credit Card Fraud Detection MLOps Pipeline

A robust end-to-end MLOps project to detect credit card fraud using Apache Airflow for orchestration, Prometheus + Grafana for monitoring, and FastAPI for real-time predictions. This project is containerized using Docker and uses AWS S3 for model artifact storage, DVC for data versioning, and Git for experiment tracking. It is deployed on AWS EC2 using Docker Compose, with container images stored on Amazon ECR and CI/CD managed by GitHub Actions.

---

## üõ†Ô∏è Tech Stack

* **MLOps Orchestration**: Apache Airflow
* **Model Serving**: FastAPI
* **Monitoring**: Prometheus, Grafana
* **Data Versioning**: DVC
* **Cloud Storage**: AWS S3
* **Modeling**: Scikit-learn
* **Experiment Tracking**: MLflow via DagsHub
* **Image Registry**: Amazon ECR
* **Infrastructure**: Docker, Docker Compose
* **CI/CD**: GitHub Actions
* **Cloud Compute**: AWS EC2

---

## üìä Project Overview

The pipeline is structured into **two main DAGs**:

### 1Ô∏è‚É£ ETL DAG (`ETL.py`)

* **Extract**: Load raw fraud transaction data.
* **Track with DVC**: Push extracted raw data to AWS S3 using DVC.
* **Transform**: Feature engineering (e.g., extract age/date-based features).
* **Load**: Save and version preprocessed data and the fitted preprocessor object.

### 2Ô∏è‚É£ Training DAG (`TRAIN.py`)

* **Drift Detection**: Compares incoming data with reference to detect feature drift.
* **Conditional Training**:

  * If **drift is detected**: ‚ùå Training is **not** triggered.
  * If **no drift detected**: ‚úÖ Model training proceeds.
* **Model Training**: Train fraud detection model.
* **Evaluation & Push**: Evaluate model and push artifacts (model + preprocessor) to S3.

---

## üìä Monitoring

This project integrates **Prometheus and Grafana**:

* Prometheus scrapes metrics from FastAPI via `/metrics` exposed using `prometheus_fastapi_instrumentator`.
* Grafana displays metrics dashboards via:

  * `datasources.yml` ‚Äì Prometheus config
  * `grafana_dash.json` ‚Äì Predefined dashboard panel for request stats, latency, etc.

---

## üåê Application Behavior

### `app.py` (FastAPI Server)

* On startup:

  * Downloads model and preprocessor from S3 (if not already present).

* On POST request from web UI:

  * Preprocesses user input using the pipeline.
  * Makes predictions using the trained model.
  * Renders prediction result in a form using `Jinja2Templates`.

---

## üìÇ Project Structure

```text
.
‚îú‚îÄ‚îÄ .dvc
‚îú‚îÄ‚îÄ airflow
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ airflow.cfg
‚îÇ   ‚îú‚îÄ‚îÄ config
‚îÇ   ‚îú‚îÄ‚îÄ dags
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bash
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dvc_load_task.sh
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dvc_track_raw_data.sh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ETL.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TRAIN.py
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îú‚îÄ‚îÄ entry.sh
‚îÇ   ‚îú‚îÄ‚îÄ plugins
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ scripts
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ data_extract.py
‚îÇ       ‚îú‚îÄ‚îÄ data_transform.py
‚îÇ       ‚îú‚îÄ‚îÄ drift_detect.py
‚îÇ       ‚îú‚îÄ‚îÄ model_evalpush.py
‚îÇ       ‚îî‚îÄ‚îÄ model_trainer.py
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ artifacts
‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fraud_data.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fraud_data.csv.dvc
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ fraud_data.zip
‚îÇ   ‚îú‚îÄ‚îÄ data_transformation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing_object
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.jbl
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.jbl.dvc
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformed
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ transformed_data.csv
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ transformed_data.csv.dvc
‚îÇ   ‚îú‚îÄ‚îÄ drift_report
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ report.yml
‚îÇ   ‚îî‚îÄ‚îÄ model_training
‚îÇ       ‚îú‚îÄ‚îÄ model.jbl
‚îÇ       ‚îî‚îÄ‚îÄ model.jbl.dvc
‚îú‚îÄ‚îÄ config
‚îÇ   ‚îú‚îÄ‚îÄ config.yml
‚îÇ   ‚îú‚îÄ‚îÄ dashboards.yml
‚îÇ   ‚îú‚îÄ‚îÄ datasources.yml
‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml
‚îú‚îÄ‚îÄ dashboards
‚îÇ   ‚îî‚îÄ‚îÄ grafana_dash.json
‚îú‚îÄ‚îÄ deploy
‚îÇ   ‚îú‚îÄ‚îÄ model.jbl
‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.jbl
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ example.env
‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ credit_card_fraud_ML_project
‚îÇ       ‚îú‚îÄ‚îÄ data
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ fraud_data.csv
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ fraud_data.csv.zip
‚îÇ       ‚îî‚îÄ‚îÄ notebook
‚îÇ           ‚îú‚îÄ‚îÄ explore.ipynb
‚îÇ           ‚îî‚îÄ‚îÄ file.py
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cloud_storage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ s3_storage.py
‚îÇ   ‚îú‚îÄ‚îÄ configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ constants
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ entity
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ artifact_entity.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_entity.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prediction_input.py
‚îÇ   ‚îú‚îÄ‚îÄ feature_transform
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ date_age.py
‚îÇ   ‚îú‚îÄ‚îÄ logger
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ utils
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ artifact_serializer.py
‚îÇ       ‚îî‚îÄ‚îÄ common.py
‚îú‚îÄ‚îÄ template
‚îÇ   ‚îî‚îÄ‚îÄ form.html
‚îú‚îÄ‚îÄ testing.py
‚îî‚îÄ‚îÄ uv.lock

```

---

## üîÑ Experiment Tracking with MLflow (via DagsHub)

This project uses **DagsHub integrated with MLflow** to track and visualize experiments:

* MLflow logs are automatically pushed to DagsHub's MLflow dashboard.
* Each training run logs:

  * Parameters
  * Metrics
  * Artifacts (model, preprocessor)


> View Experiments on DagsHub: `https://dagshub.com/<your-username>/<your-repo>/experiments`

---

## üö¢ Deployment Overview

The system is fully production-ready and runs on AWS EC2.

### üèê GitHub Actions (CI/CD)

* Triggered on push to `main`
* Builds Docker images for all services (Airflow, FastAPI, etc.)
* Pushes them to **Amazon ECR**
* SSH into EC2 to pull and restart containers using `docker-compose`

### üìÅ Amazon ECR

Stores Docker images for:

* Airflow
* FastAPI
* Prometheus
* Grafana

### üåé AWS EC2

Runs the full MLOps stack using `docker-compose` with port exposure for:

* Airflow: `http://<ec2-ip>:8080`
* FastAPI: `http://<ec2-ip>:8000`
* Grafana: `http://<ec2-ip>:3000`

---

## üö≤ Deployment Architecture

```
+-------------+       Push to main       +---------------------+
| Developer   |  --------------------->  | GitHub Actions CI/CD|
+-------------+                          +---------------------+
                                                  |
                                                  | Builds Docker image
                                                  v
                                        +----------------------+
                                        | Amazon ECR (Docker)  |
                                        +----------------------+
                                                  |
                                                  | SSH + Pull + Restart
                                                  v
                                     +-----------------------------+
                                     | AWS EC2                     |
                                     |   docker-compose up         |
                                     +-----------------------------+
                                                  |
             +-------------+----------+------------+-------------+
             |             |          |            |             |
        Airflow       FastAPI     Prometheus    Grafana      MLflow UI (DagsHub)
      (port 8080)    (port 8000)   (port 9090)   (port 3000)   (via web link)
```

---

## ‚öôÔ∏è Run Locally

```bash
# Clone the repo
git clone https://github.com/Manyfaces860/ETE_MLOps_Credit_Card_Fraud_Prediction.git
cd ETE_MLOps_Credit_Card_Fraud_Prediction

# Set environment variables in .env
cp example.env .env

# Spin up everything
# set AIRFLOW_UID and give necessary permissions to airflow for successfully running airflow container
docker compose up --build -d

# Access UIs
# Airflow:   http://localhost:8080
# FastAPI:   http://localhost:8000
# Grafana:   http://localhost:3000
```

---

## üõ°Ô∏è Secrets Required (GitHub Actions)

`AWS_ACCESS_KEY_ID`  
`AWS_SECRET_ACCESS_KEY`
`AIRFLOW_UID`
`GIT_EMAIL`
`GIT_NAME`
`DVC_REMOTE_NAME`
`DVC_S3_BUCKET`
`AWS_DEFAULT_REGION`

---
